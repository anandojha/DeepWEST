{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "import itertools\n",
    "import DeepWEST \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data ( .prmtop and .nc should be present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/aaojha/data_for_deepwest/alanine_dipeptide\"\n",
    "traj_file = os.path.join(data_dir, \"alanine_dipeptide_250ns_250000steps.nc\")\n",
    "top = os.path.join(data_dir, \"alanine_dipeptide.prmtop\")\n",
    "heavy_atoms_file = os.path.join(\"heavy_atoms_md.txt\")\n",
    "phi_psi_file = os.path.join(\"phi_psi_md.txt\")\n",
    "DeepWEST.create_heavy_atom_xyz_solvent(traj = traj_file, top = top, heavy_atoms_array = heavy_atoms_file, \n",
    "                                       start = 0, stop = 250000, stride = 1)\n",
    "DeepWEST.create_phi_psi_solvent_alanine_dipeptide(traj = traj_file, top = top, phi_psi_txt = phi_psi_file, \n",
    "                                                  start = 0, stop = 250000, stride = 1)\n",
    "traj_whole = np.loadtxt(heavy_atoms_file)\n",
    "print(traj_whole.shape)\n",
    "dihedral = np.loadtxt(phi_psi_file)\n",
    "print(dihedral.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1 # Tau, how much is the timeshift of the two datasets\n",
    "batch_size = 1000 # Batch size for Stochastic Gradient descent\n",
    "train_ratio = 0.9 # Which trajectory points percentage is used as training\n",
    "network_depth = 6 # How many hidden layers the network has\n",
    "layer_width = 100 # Width of every layer\n",
    "learning_rate = 1e-4 # Learning rate used for the ADAM optimizer\n",
    "#output_size = 6 # How many output states the network has\n",
    "output_size = 3 # How many output states the network has\n",
    "nb_epoch = 60 # Iteration over the training set in the fitting process\n",
    "nb_epoch = 100 # Iteration over the training set in the fitting process\n",
    "epsilon = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_data_points, input_size = traj_whole.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialized the VAMPnets wrapper class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vamp = DeepWEST.VampnetTools(epsilon = epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle trajectory and lagged trajectory together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_data = traj_data_points - tau\n",
    "traj_ord = traj_whole[:length_data]\n",
    "traj_ord_lag = traj_whole[tau:length_data+tau]\n",
    "dihedral_init = dihedral[:length_data]\n",
    "indexes = np.arange(length_data)\n",
    "np.random.shuffle(indexes)\n",
    "traj = traj_ord[indexes]\n",
    "traj_lag = traj_ord_lag[indexes]\n",
    "dihedral_shuffle = dihedral_init[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare data for tensorflow usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_train = int(np.floor(length_data * train_ratio))\n",
    "length_vali = length_data - length_train\n",
    "traj_data_train = traj[:length_train]\n",
    "traj_data_train_lag = traj_lag[:length_train]\n",
    "traj_data_valid = traj[length_train:]\n",
    "traj_data_valid_lag = traj_lag[length_train:]\n",
    "# Input of the first network\n",
    "X1_train = traj_data_train.astype('float32')\n",
    "X2_train  = traj_data_train_lag.astype('float32')\n",
    "# Input for validation\n",
    "X1_vali = traj_data_valid.astype('float32')\n",
    "X2_vali = traj_data_valid_lag.astype('float32')\n",
    "# Needs a Y-train set which we dont have.\n",
    "Y_train = np.zeros((length_train,2*output_size)).astype('float32')\n",
    "Y_vali = np.zeros((length_vali,2*output_size)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run several model iterations saving the best one, to help finding sparcely populated states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_vm = 0\n",
    "attempts = 10\n",
    "losses = [vamp.loss_VAMP2_autograd]\n",
    "for i in range(attempts):    \n",
    "    # Clear the previous tensorflow session to prevent memory leaks\n",
    "    #clear_session()\n",
    "    tf.keras.backend.clear_session()\n",
    "    # Build the model\n",
    "    nodes = [layer_width]*network_depth\n",
    "    Data_X = tf.keras.layers.Input(shape = (input_size,))\n",
    "    Data_Y = tf.keras.layers.Input(shape = (input_size,))\n",
    "    # Batch normalization layer improves convergence speed\n",
    "    bn_layer = tf.keras.layers.BatchNormalization()\n",
    "    # Instance layers and assign them to the two lobes of the network\n",
    "    dense_layers = [tf.keras.layers.Dense(node, activation = 'elu') \n",
    "                    # if index_layer < 3 else 'linear nodes')\n",
    "                    for index_layer,node in enumerate(nodes)]\n",
    "    lx_branch = bn_layer(Data_X)\n",
    "    rx_branch = bn_layer(Data_Y)\n",
    "    for i, layer in enumerate(dense_layers):\n",
    "        lx_branch = dense_layers[i](lx_branch)\n",
    "        rx_branch = dense_layers[i](rx_branch)\n",
    "    # Add a softmax output layer\n",
    "    # Should be replaced with a linear activation layer if\n",
    "    # the outputs of the network cannot be interpreted as states\n",
    "    softmax = tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "    lx_branch = softmax(lx_branch)\n",
    "    rx_branch = softmax(rx_branch)\n",
    "    # Merge both networks to train both at the same time\n",
    "    merged = tf.keras.layers.concatenate([lx_branch, rx_branch])\n",
    "    # Initialize the model and the optimizer, and compile it with\n",
    "    # the loss and metric functions from the VAMPnets package\n",
    "    model = tf.keras.models.Model(inputs = [Data_X, Data_Y], outputs = merged)\n",
    "    adam = tf.keras.optimizers.Adam(learning_rate = learning_rate/10)\n",
    "    vm1 = np.zeros((len(losses), nb_epoch))\n",
    "    tm1 = np.zeros_like(vm1)\n",
    "    vm2 = np.zeros_like(vm1)\n",
    "    tm2 = np.zeros_like(vm1)\n",
    "    vm3 = np.zeros_like(vm1)\n",
    "    tm3 = np.zeros_like(vm1)\n",
    "    for l_index, loss_function in enumerate(losses):\n",
    "        model.compile(optimizer = adam,\n",
    "                      loss = loss_function,\n",
    "                      metrics = [vamp.metric_VAMP, vamp.metric_VAMP2])\n",
    "        # Train the model  \n",
    "        hist = model.fit([X1_train, X2_train], Y_train ,\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=nb_epoch,\n",
    "                         validation_data=([X1_vali, X2_vali], Y_vali ),\n",
    "                         verbose=0)\n",
    "        vm1[l_index] = np.array(hist.history['val_metric_VAMP'])\n",
    "        tm1[l_index] = np.array(hist.history['metric_VAMP'])\n",
    "        vm2[l_index] = np.array(hist.history['val_metric_VAMP2'])\n",
    "        tm2[l_index] = np.array(hist.history['metric_VAMP2'])\n",
    "        vm3[l_index] = np.array(hist.history['val_loss'])\n",
    "        tm3[l_index] = np.array(hist.history['loss'])\n",
    "    vm1 = np.reshape(vm1, (-1))\n",
    "    tm1 = np.reshape(tm1, (-1))\n",
    "    vm2 = np.reshape(vm2, (-1))\n",
    "    tm2 = np.reshape(tm2, (-1))\n",
    "    vm3 = np.reshape(vm3, (-1))\n",
    "    tm3 = np.reshape(tm3, (-1))\n",
    "    # Average the score obtained in the last part of the training process\n",
    "    # in order to establish which model is better and thus worth saving\n",
    "    score = vm1[-5:].mean()\n",
    "    extra_msg = ''\n",
    "    if score > max_vm:\n",
    "        extra_msg = ' - Highest'\n",
    "        best_weights = model.get_weights()\n",
    "        max_vm = score\n",
    "        vm1_max = vm1\n",
    "        tm1_max = tm1\n",
    "        vm2_max = vm2\n",
    "        tm2_max = tm2\n",
    "        vm3_max = vm3\n",
    "        tm3_max = tm3  \n",
    "    print('Score: {0:.2f}'.format(score) + extra_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recover the saved model and its training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(best_weights)\n",
    "tm1 = np.array(tm1_max)\n",
    "tm2 = np.array(tm2_max)\n",
    "tm3 = np.array(tm3_max)\n",
    "vm1 = np.array(vm1_max)\n",
    "vm2 = np.array(vm2_max)\n",
    "vm3 = np.array(vm3_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training result visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(vm1, label = 'VAMP')\n",
    "plt.plot(vm2, label = 'VAMP2')\n",
    "plt.plot(-vm3, label = 'loss')\n",
    "plt.plot(tm1, label = 'training VAMP')\n",
    "plt.plot(tm2, label = 'training VAMP2')\n",
    "plt.plot(-tm3, label = 'training loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the input trajectory using the network\n",
    "states_prob = model.predict([traj_ord, traj_ord_lag])[:, :output_size]\n",
    "# Order the output states based on their population\n",
    "coor_pred = np.argmax(states_prob, axis = 1)\n",
    "indexes = [np.where(coor_pred == np.multiply(np.ones_like(coor_pred), n)) \n",
    "           for n in range(output_size)]\n",
    "states_num = [len(i[0]) for i in indexes]\n",
    "states_order = np.argsort(states_num).astype('int')[::-1]\n",
    "pred_ord = states_prob[:,states_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the population of the states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_states_pie_chart():\n",
    "    coors = []\n",
    "    maxi = np.max(pred_ord, axis= 1)\n",
    "    for i in range(output_size):\n",
    "        coors.append(len(np.where(pred_ord[:,i] == maxi)[0]))\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.pie(np.array(coors), autopct='%1.2f%%', startangle=90)\n",
    "    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    print('States population: '+ str(np.array(coors)/len(maxi)*100)+'%')\n",
    "    plt.show()\n",
    "print_states_pie_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize how the states are placed on the Ramachandran plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maxi_train = np.max(pred_ord, axis= 1)\n",
    "coor_train = np.zeros_like(pred_ord)\n",
    "for i in range(output_size):\n",
    "    coor_train = np.where(pred_ord[:,i]== maxi_train)[0]\n",
    "    plt.scatter(dihedral_init[coor_train,0], dihedral_init[coor_train,1], s=5)\n",
    "plt.axes = [[-np.pi, np.pi],[-np.pi, np.pi]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each state, visualize the probabilities the different trajectory points have to belong to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,25))\n",
    "gs1 = gridspec.GridSpec(2, int(np.ceil(output_size/2)))\n",
    "gs1.update(wspace=0.05, hspace = 0.05)\n",
    "for n in range(output_size):\n",
    "    ax = plt.subplot(gs1[n])\n",
    "    im = ax.scatter(dihedral_init[:,0], dihedral_init[:,1], s=30,\n",
    "                    c = pred_ord[:,n],\n",
    "                    alpha=0.5,\n",
    "                    vmin = 0, vmax = 1\n",
    "                    )\n",
    "    plt.axis('on')\n",
    "    title = 'State '+str(n + 1)\n",
    "    ax.text(.85, .15, title,\n",
    "        horizontalalignment='center',\n",
    "        transform=ax.transAxes,  fontdict = {'size':36})\n",
    "    if (n < 3):\n",
    "        ax.set_xticks([-3, 0, 3])\n",
    "        ax.set_xticklabels([r'-$\\pi$', r'$0$', r'$\\pi$'])\n",
    "        ax.xaxis.set_tick_params(top='on', bottom='off', labeltop='on', labelbottom='off')\n",
    "        ax.xaxis.set_tick_params(labelsize=40)\n",
    "    else:\n",
    "        ax.set_xticks([])\n",
    "    if (n%3==0):\n",
    "        ax.set_yticks([-3, 0, 3])\n",
    "        ax.set_yticklabels([r'-$\\pi$', r'$0$', r'$\\pi$'])\n",
    "        ax.yaxis.set_tick_params(labelsize=40)\n",
    "    else:\n",
    "        ax.set_yticks([])\n",
    "#    ax.set_aspect('equal')\n",
    "    ax.set_xlim([-np.pi, np.pi]);\n",
    "    ax.set_ylim([-np.pi, np.pi]);\n",
    "    if (n%3 == 0):\n",
    "        ax.set_ylabel(r'$\\Psi$ [rad]', fontdict = {'size':40})\n",
    "    if (n < 3):\n",
    "        ax.set_xlabel(r'$\\Phi$ [rad]', fontdict = {'size':40}, position = 'top')\n",
    "        ax.xaxis.set_label_coords(0.5,1.2)\n",
    "gs1.tight_layout(fig, rect=[0, 0.03, 0.95, 0.94])\n",
    "fig.show()\n",
    "cax = fig.add_axes([0.95, 0.05, 0.02, 0.8])\n",
    "cbar = fig.colorbar(im, cax=cax, ticks=[0, 1])\n",
    "cbar.ax.yaxis.set_tick_params(labelsize=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Markov Model Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate the implied timescales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_tau = 200\n",
    "lag = np.arange(1, max_tau, 1)\n",
    "its = vamp.get_its(pred_ord, lag)\n",
    "vamp.plot_its(its, lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chapman-Kolmogorov test for the estimated koopman operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 8\n",
    "tau_msm = 35\n",
    "predicted, estimated = vamp.get_ck_test(pred_ord, steps, tau_msm)\n",
    "vamp.plot_ck_test(predicted, estimated, output_size, steps, tau_msm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the frame indices to a txt file\n",
    "index_for_we = []\n",
    "for i in indexes:\n",
    "    index_frames = list(list(i)[0])\n",
    "    sel_frames = index_frames[:10]\n",
    "    index_for_we.append(sel_frames)\n",
    "index_for_we = list(itertools.chain.from_iterable(index_for_we))\n",
    "print(len(index_for_we))\n",
    "np.savetxt(\"indices_vamp.txt\", index_for_we)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
